{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b1dbdca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T12:00:35.280918Z",
     "start_time": "2023-01-11T12:00:35.249127Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/82103/Py_3_10/deep-learning-from-scratch\")\n",
    "from common.functions import *\n",
    "from dataset.mnist import load_mnist\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e96b879",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T10:30:53.592193Z",
     "start_time": "2023-01-11T10:30:53.565556Z"
    }
   },
   "outputs": [],
   "source": [
    "# 사과 쇼핑 계산 그래프 예제에서 모든 계층을 각각 다른 class로 구현\n",
    "# 곱셈 계층 구현\n",
    "# 순전파 계산과 역전파 계산을 하는 함수를 가지고 있다\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # 입력값을 객체의 멤버 변수로 저장 -> 역전파 시 사용\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        out = x * y\n",
    "        return out\n",
    "    \n",
    "    # dout은 다음 계층에서 건네받은 편미분값\n",
    "    def backward(self, dout):\n",
    "        # 두개의 입력 변수에 대한 각각의 편미분 값을 생성\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce72f68a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T10:30:54.048073Z",
     "start_time": "2023-01-11T10:30:54.026776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n",
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "apple = 100 # 사과의 가격\n",
    "apple_num = 2 # 사과의 개수\n",
    "tax = 1.1 # 소비세\n",
    "\n",
    "firstLayer = MulLayer()\n",
    "secondLayer = MulLayer()\n",
    "\n",
    "# forward계산\n",
    "apple_price = firstLayer.forward(apple, apple_num)\n",
    "price = secondLayer.forward(apple_price, tax)\n",
    "print(price)\n",
    "\n",
    "# backward계산\n",
    "dprice = 1\n",
    "dapple_price, dtax = secondLayer.backward(dprice)\n",
    "dapple, dapple_num = firstLayer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1fbb83c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T10:30:54.522131Z",
     "start_time": "2023-01-11T10:30:54.506515Z"
    }
   },
   "outputs": [],
   "source": [
    "# 덧셈 계층 구현\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    # 덧셈 노드에서 역전파를 계산할 때 입력값 크게 영향 x\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "216f4915",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T10:30:54.977931Z",
     "start_time": "2023-01-11T10:30:54.943963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "2.2 110.00000000000001 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "appleLayer = MulLayer()\n",
    "orangeLayer = MulLayer()\n",
    "addLayer = AddLayer()\n",
    "taxLayer = MulLayer()\n",
    "\n",
    "# forward계산\n",
    "# 계산값을 다음층으로 순차적으로 넘겨주면서 계산\n",
    "apple_price = appleLayer.forward(apple, apple_num)\n",
    "orange_price = orangeLayer.forward(orange, orange_num)\n",
    "sum_price = addLayer.forward(apple_price, orange_price)\n",
    "price = taxLayer.forward(sum_price, tax)\n",
    "print(price)\n",
    "\n",
    "# backward계산\n",
    "dprice = 1\n",
    "dsum_price, dtax = taxLayer.backward(dprice)\n",
    "dapple_price, dorange_price = addLayer.backward(dsum_price)\n",
    "dapple, dapple_num = appleLayer.backward(dapple_price)\n",
    "dorange, dorange_num = orangeLayer.backward(dorange_price)\n",
    "\n",
    "print(dapple, dapple_num, dorange, dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba234afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T11:52:08.092813Z",
     "start_time": "2023-01-11T11:52:08.065880Z"
    }
   },
   "outputs": [],
   "source": [
    "# 활성화 함수의 계층 구현하기\n",
    "# Relu함수\n",
    "# 함수의 입력값이 <=0이면 0으로 출력해주는 특징이 있다.\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 넘파이의 마스킹 연산을 이용해서 입력 배열 중 0보다 작은 원소의 인덱스에는 True를 저장, \n",
    "        # 0보다 큰 원소의 인덱스에는 False를 저장한 마스크를 생성한다,\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0 # True인 부분 즉, 입력 배열의 원소 중 0보다 작은 부분을 0으로 만들어준다.\n",
    "        return out\n",
    "    \n",
    "    # Relu함수를 편미분 했을 때 1 또는 0\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout * 1 \n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83c3ac91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T10:43:24.167706Z",
     "start_time": "2023-01-11T10:43:24.142101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sigmoid함수\n",
    "# 계산 그래프에서 세부 계층으로 나눈 뒤 역전파를 파악함에 주의\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out # 순전파의 출력값 저장 -> 역전파에 사용하기 위함\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a218f28d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T10:30:56.915098Z",
     "start_time": "2023-01-11T10:30:56.883911Z"
    }
   },
   "outputs": [],
   "source": [
    "# Affine 계층 구현 -> 행렬곱연산\n",
    "# 배치용 Affine으로 구현하되 배치 수 N = 2라고 가정한다(데이터가 2개)\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T) # self.W.T는 self.W의 transpose를 의미한다\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "688078f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T11:49:59.302465Z",
     "start_time": "2023-01-11T11:49:59.269979Z"
    }
   },
   "outputs": [],
   "source": [
    "# 출력층의 활성화함수인 Softmax함수와 loss함수를 한 계층으로 묶어서 생각\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.y = softmax(x)\n",
    "        self.t = t\n",
    "        self.loss = cross_entropy_error(self.y, t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx # 역전파 값을 담은 넘파이 배열 반환       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "475ddffe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T12:00:38.891712Z",
     "start_time": "2023-01-11T12:00:38.841912Z"
    }
   },
   "outputs": [],
   "source": [
    "# 코드가 깔끔하진 않지만 gradient의 구현은 대략적으로 맞은듯??\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {} # 파라미터들을 담을 딕셔너리 선언\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        self.hidden1_affine_layer = None\n",
    "        self.hidden1_relu_layer = None\n",
    "        self.hidden2_affine_layer = None\n",
    "        self.last_layer = None\n",
    "        \n",
    "    # 정의해 두었던 계층을 이용한 순전파 처리    \n",
    "    def predict(self, x):\n",
    "        W1, b1, W2, b2 = self.params['W1'], self.params['b1'], self.params['W2'], self.params['b2']\n",
    "        self.hidden1_affine_layer = Affine(W1, b1)\n",
    "        self.hidden1_relu_layer = Relu()\n",
    "        self.hidden2_affine_layer = Affine(W2, b2)\n",
    "        \n",
    "        a1 = self.hidden1_affine_layer.forward(x)\n",
    "        z1 = self.hidden1_relu_layer.forward(a1)\n",
    "        \n",
    "        a2 = self.hidden2_affine_layer.forward(z1)\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "            \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    # 수치 미분을 통한 기울기 연산\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "\n",
    "        \n",
    "    # 정의해 두었던 계층을 이용한 역전파 처리\n",
    "    def gradient(self, x, t):\n",
    "        grads = {}\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        y = self.predict(x)\n",
    "        loss = self.last_layer.forward(y, t)\n",
    "        \n",
    "        dloss = self.last_layer.backward()\n",
    "        da2 = self.hidden2_affine_layer.backward(dloss)\n",
    "        \n",
    "        grads['W2'] = self.hidden2_affine_layer.dW\n",
    "        grads['b2'] = self.hidden2_affine_layer.db\n",
    "        \n",
    "        dz1 = self.hidden1_relu_layer.backward(da2)\n",
    "        da1 = self.hidden1_affine_layer.backward(dz1)\n",
    "        \n",
    "        grads['W1'] = self.hidden1_affine_layer.dW\n",
    "        grads['b1'] = self.hidden1_affine_layer.db\n",
    "        \n",
    "        return grads\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0602a073",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T12:00:39.824383Z",
     "start_time": "2023-01-11T12:00:39.372798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cae69d80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T12:00:40.419304Z",
     "start_time": "2023-01-11T12:00:40.048198Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m t_train[:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m      5\u001b[0m grad_backprop \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mgradient(x_batch, t_batch)\n\u001b[1;32m----> 6\u001b[0m grad_nu \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m grad_nu\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m      9\u001b[0m     diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(np\u001b[38;5;241m.\u001b[39mabs(grad_backprop[key] \u001b[38;5;241m-\u001b[39m grad_nu[key]))\n",
      "Cell \u001b[1;32mIn[37], line 39\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     36\u001b[0m loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[0;32m     38\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 39\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     41\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mC:\\Users/82103/Py_3_10/deep-learning-from-scratch\\common\\gradient.py:43\u001b[0m, in \u001b[0;36mnumerical_gradient\u001b[1;34m(f, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m tmp_val \u001b[38;5;241m=\u001b[39m x[idx]\n\u001b[0;32m     42\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(tmp_val) \u001b[38;5;241m+\u001b[39m h\n\u001b[1;32m---> 43\u001b[0m fxh1 \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# f(x+h)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m tmp_val \u001b[38;5;241m-\u001b[39m h \n\u001b[0;32m     46\u001b[0m fxh2 \u001b[38;5;241m=\u001b[39m f(x) \u001b[38;5;66;03m# f(x-h)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[37], line 36\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[1;34m(W)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 36\u001b[0m     loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     39\u001b[0m     grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[37], line 30\u001b[0m, in \u001b[0;36mTwoLayerNet.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 30\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m(x)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_entropy_error(y, t)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "grad_nu = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_nu.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_nu[key]))\n",
    "    print(key + \":\" +str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eed289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

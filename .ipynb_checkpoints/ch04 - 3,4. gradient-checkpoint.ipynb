{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3c3194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/82103/Py_3_10/deep-learning-from-scratch\")\n",
    "from common.functions import softmax, cross_entropy_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f70e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미분 함수 구현\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x + h) - f(x - h)) / (2*h)\n",
    "\n",
    "# 변수가 2개인 함수\n",
    "def function_2(x):\n",
    "    return np.sum(x**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6138288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다변수 함수에 대한 기울기 값 계산 -> 각 변수에 대한 편미분 값을 계산 후 벡터로 표현\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x + h)의 함수값 계산 (idx번째 변수x + h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e4115fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 X의 차원에 따른 구분을 해준 기울기 구하는 함수\n",
    "def numerical_gradient(f, X):\n",
    "    # X가 1차원일 경우\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    # X가 다차원일 경우\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "        \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c910d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b10ebba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.11110793e-10  8.14814391e-10]\n",
      "1.0373788922158197e-18\n"
     ]
    }
   ],
   "source": [
    "# function_2(x)에 대해 최소가 되는 x값을 찾고 최솟값을 구하자\n",
    "# 시작 x값 설정 (x0, x1)\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "x = gradient_descent(function_2, init_x, 0.1, 100)\n",
    "print(x)\n",
    "y = function_2(x)\n",
    "print(y)\n",
    "\n",
    "# lr값을 어떻게 설정하느냐에 따라 최소가 되는 값이 매우 다양하게 바뀜\n",
    "# -> 적당한 lr값 설정하는 것도 중요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea65e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        # 가중치 값 정규분포로 임의로 초기화 (2x3 행렬형태)\n",
    "        self.W = np.random.randn(2, 3)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    # 임의의 가중치에 대한 출력에 대한 손실함수까지 계산해서 반환\n",
    "    def loss(self, x, t):\n",
    "        # 단층 퍼셉트론 형태의 신경망 구현(은닉층 존재 x)\n",
    "        z = self.predict(x)\n",
    "        # 활성화 함수로 softmax함수 사용 -> 사용효과 : 출력값이 확률의 의미를 가짐\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4074d409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.22663884  0.22474645  0.3307797 ]\n",
      " [-0.92844528 -1.50658779 -0.2961398 ]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c532b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.97158406 -1.22108114 -0.068058  ]\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5428005028842243"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "print(np.argmax(p))\n",
    "# 정답 레이블\n",
    "t = np.array([0, 0, 1])\n",
    "# 임의의 가중치에 대한 출력\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e48cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14126008  0.1100688  -0.25132889]\n",
      " [ 0.21189013  0.16510321 -0.37699333]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "# 각 가중치에 대한 편미분값을 나타낸 기울기 dW \n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f252d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
